/*
 * Copyright 2017 LinkedIn Corp. Licensed under the BSD 2-Clause License (the "License"). See License in the project root for license information.
 */

package com.linkedin.kafka.cruisecontrol.monitor.sampling.aggregator;

import com.linkedin.cruisecontrol.monitor.sampling.Snapshot;
import com.linkedin.cruisecontrol.monitor.sampling.aggregator.MetricSampleAggregator;
import com.linkedin.cruisecontrol.monitor.sampling.aggregator.SnapshotAndImputation;
import com.linkedin.cruisecontrol.monitor.sampling.aggregator.WindowAggregationResult;
import com.linkedin.kafka.cruisecontrol.common.MetadataClient;
import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfig;
import com.linkedin.cruisecontrol.resource.Resource;
import com.linkedin.cruisecontrol.exception.NotEnoughSnapshotsException;
import com.linkedin.kafka.cruisecontrol.exception.NotEnoughValidSnapshotsException;
import com.linkedin.kafka.cruisecontrol.monitor.ModelCompletenessRequirements;
import com.linkedin.kafka.cruisecontrol.monitor.sampling.PartitionMetricSample;
import com.linkedin.kafka.cruisecontrol.monitor.sampling.aggregator.KafkaMetricSampleAggregatorMetadata.AvailableWindows;
import java.util.HashSet;
import java.util.Set;
import java.util.SortedMap;
import java.util.SortedSet;
import org.apache.kafka.clients.Metadata;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.Node;
import org.apache.kafka.common.PartitionInfo;
import org.apache.kafka.common.TopicPartition;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.List;

/**
 * This class aggregates the metric samples generated by the MetricFetcher.
 * <p>
 * The metric sample aggregator performs the sanity check on the samples and aggregate the samples into the
 * corresponding snapshot window. Because the Kafka broker does not report the bytes in rate for the followers,
 * we assume the sample we get are only from the leaders, and we are going to derive the follower metrics based on
 * the leader metrics.
 * </p>
 */
public class KafkaMetricSampleAggregator extends MetricSampleAggregator<TopicPartition> {
  private static final Logger LOG = LoggerFactory.getLogger(KafkaMetricSampleAggregator.class);
  // TODO: Currently we consider all the imputations as valid, will limit the max allowed imputations later.
  private static final float MAX_SNAPSHOT_PERCENT_WITH_FLAWS = 0.1f;
  private final Metadata _metadata;
  private volatile KafkaMetricSampleAggregatorMetadata _aggregatorMetadata;

  /**
   * Construct the metric sample aggregator.
   *
   * @param config   The load monitor configurations.
   * @param metadata The metadata of the cluster.
   */
  public KafkaMetricSampleAggregator(KafkaCruiseControlConfig config,
                                     Metadata metadata) {
    super(config.getInt(KafkaCruiseControlConfig.NUM_LOAD_SNAPSHOTS_CONFIG),
          config.getLong(KafkaCruiseControlConfig.LOAD_SNAPSHOT_WINDOW_MS_CONFIG),
          config.getInt(KafkaCruiseControlConfig.MIN_SAMPLES_PER_LOAD_SNAPSHOT_CONFIG));
    _metadata = metadata;
    double maxAllowedImputations =
        Math.max(1, config.getInt(KafkaCruiseControlConfig.NUM_LOAD_SNAPSHOTS_CONFIG) * MAX_SNAPSHOT_PERCENT_WITH_FLAWS);
    _aggregatorMetadata = new KafkaMetricSampleAggregatorMetadata(-1L, maxAllowedImputations);
  }

  /**
   * Add a sample to the metric aggregator. This method is thread safe.
   *
   * @param sample The metric sample to add.
   */
  public boolean addSample(PartitionMetricSample sample) {
    return addSample(sample, true);
  }

  /**
   * Add a sample to the metric aggregator. This method is thread safe.
   *
   * @param sample The metric sample to add.
   * @param leaderValidation whether perform the leader validation or not.
   *
   * @return true if the sample is accepted, false if the sample is ignored.
   */
  public boolean addSample(PartitionMetricSample sample, boolean leaderValidation) {
    // Sanity check the sample
    return isValidSample(sample, leaderValidation) && super.addSample(sample);
  }

  /**
   * Collect the snapshots for all the topic partitions.
   * <p>
   * If a topic has at least one snapshot that does not have enough samples, that topic will be excluded from the
   * returned snapshot. This is because:
   * <ol>
   * <li>We assume that only new topics would have insufficient data. So we only balance the existing topics and
   * allow more time to collect enough utilization data for the new topics.</li>
   * <li>If we don't have enough data to make a replica movement decision, it is better not to take any action.</li>
   * </ol>
   *
   * @param clusterAndGeneration The current cluster information.
   * @param now the current time.
   * @return A mapping between the partition info and the snapshots.
   */
  public MetricSampleAggregationResult recentSnapshots(MetadataClient.ClusterAndGeneration clusterAndGeneration,
                                                       long now)
      throws NotEnoughSnapshotsException {
    try {
      ModelCompletenessRequirements requirements = new ModelCompletenessRequirements(1, 0.0, false);
      return snapshots(clusterAndGeneration, -1L, now, requirements);
    } catch (NotEnoughValidSnapshotsException e) {
      throw new IllegalStateException("Should not throw NotEnoughValidSnapshotsException here.", e);
    }
  }

  /**
   * Collect the snapshots for all the topic partitions for a time window.
   * <p>
   * If a topic has at least one snapshot that does not have enough samples, that topic will be excluded from the
   * returned snapshot. This is because:
   * <ol>
   * <li>We assume that only new topics would have insufficient data. So we only balance the existing topics and
   * allow more time to collect enough utilization data for the new topics.</li>
   * <li>If we don't have enough data to make a replica movement decision, it is better not to take any action.</li>
   * </ol>
   *
   * @param clusterAndGeneration The current cluster information.
   * @param from the start of the time window
   * @param to the end of the time window
   * @param completenessRequirements the {@link ModelCompletenessRequirements} for the aggregation result.
   * @return A mapping between the partition info and the snapshots.
   */
  public MetricSampleAggregationResult snapshots(MetadataClient.ClusterAndGeneration clusterAndGeneration,
                                                 long from,
                                                 long to,
                                                 ModelCompletenessRequirements completenessRequirements)
      throws NotEnoughSnapshotsException, NotEnoughValidSnapshotsException {
    long generation = currentGeneration();
    int requiredNumSnapshots = completenessRequirements.minRequiredNumSnapshotWindows();

    Set<TopicPartition> allPartitions = allPartitions(clusterAndGeneration.cluster());
    List<WindowAggregationResult<TopicPartition>> windowResults =
        super.windowAggregationResults(from, to, allPartitions);
    if (windowResults.size() < requiredNumSnapshots) {
      throw new NotEnoughSnapshotsException(String.format("Required %d snapshot windows but there are only %d "
                                                              + "snapshot windows available.",
                                                          requiredNumSnapshots, windowResults.size()));
    }
    KafkaMetricSampleAggregatorMetadata aggregatorMetadata =
        updateAggregatorMetadataIfNeeded(clusterAndGeneration, windowResults);
    AvailableWindows availableWindows = 
        filterWindowAggregationResults(aggregatorMetadata, windowResults, completenessRequirements, allPartitions.size());
    Set<String> validTopics = availableWindows.validTopics();
    MetricSampleAggregationResult result = new MetricSampleAggregationResult(generation);
    for (TopicPartition tp : allPartitions) {
      addPartitionToResult(result, windowResults, tp, completenessRequirements, 
                           aggregatorMetadata.topicsWithTooManyImputations(), validTopics);
    }
    return result;
  }

  /**
   * Get a sorted set of valid snapshot windows in the aggregator. A valid snapshot window is a window with enough
   * valid partitions being monitored. A valid partition must be valid in all the windows in the returned set.
   *
   * @param clusterAndGeneration The current cluster and generation.
   * @param minMonitoredPartitionsPercentage the minimum required monitored partitions percentage.
   * @param totalNumPartitions the total number of of partitions in the cluster.
   * @return a sorted set of valid snapshot windows in the aggregator.
   */
  public SortedSet<Long> validSnapshotWindows(MetadataClient.ClusterAndGeneration clusterAndGeneration,
                                              double minMonitoredPartitionsPercentage,
                                              int totalNumPartitions) {
    updateAggregatorMetadataIfNeeded(clusterAndGeneration);
    return _aggregatorMetadata.validWindows(minMonitoredPartitionsPercentage, totalNumPartitions).availableWindows();
  }

  /**
   * Get the valid partitions percentage across all the windows.
   *
   * @param clusterAndGeneration the current cluster and generation.
   * @param totalNumPartitions the total number of partitions in the cluster.
   * @return The percentage of valid partitions acorss all the windows.
   */
  public double monitoredPercentage(MetadataClient.ClusterAndGeneration clusterAndGeneration,
                                     int totalNumPartitions) {
    updateAggregatorMetadataIfNeeded(clusterAndGeneration);
    return (double) _aggregatorMetadata.numValidPartitions() / totalNumPartitions;
  }

  /**
   * Get the monitored partition percentage in each window.
   * @param clusterAndGeneration the current cluster and generation.
   * @param totalNumPartitions the total number of partitions in the cluster.
   * @return A mapping from window to the monitored partitions percentage.
   */
  public SortedMap<Long, Double> monitoredPercentagesByWindows(MetadataClient.ClusterAndGeneration clusterAndGeneration,
                                                               int totalNumPartitions) {
    updateAggregatorMetadataIfNeeded(clusterAndGeneration);
    return _aggregatorMetadata.monitoredPercentagesByWindows(totalNumPartitions);
  }

  /**
   * Filter out the window aggregation results that failed the min monitored partition percentage requirement.
   * If there are not enough snapshot window left to meet the min required snapshots requirements, an exception
   * will be thrown.
   *
   * @param windowResults the list of window aggregation result to filter.
   * @param requirements the model completeness requirements.
   */
  private AvailableWindows filterWindowAggregationResults(KafkaMetricSampleAggregatorMetadata aggregatorMetadata,
                                                          List<WindowAggregationResult<TopicPartition>> windowResults,
                                                          ModelCompletenessRequirements requirements,
                                                          int totalNumPartitions)
      throws NotEnoughValidSnapshotsException {
    AvailableWindows availableWindows = aggregatorMetadata.validWindows(requirements.minMonitoredPartitionsPercentage(), totalNumPartitions);
    windowResults.removeIf(windowResult -> !availableWindows.availableWindows().contains(windowResult.window()));
    if (windowResults.size() < requirements.minRequiredNumSnapshotWindows()) {
      throw new NotEnoughValidSnapshotsException("Required " + requirements.minRequiredNumSnapshotWindows() + " snapshot windows "
                                                + "with " + requirements.minMonitoredPartitionsPercentage() + " of monitored "
                                                + "partitions, but there are only " + windowResults.size() + " available");
    }
    return availableWindows;
  }

  private void updateAggregatorMetadataIfNeeded(MetadataClient.ClusterAndGeneration clusterAndGeneration) {
    synchronized (this) {
      if (_aggregatorMetadata == null || !_aggregatorMetadata.generationsMatch(currentGeneration(), clusterAndGeneration.generation())) {
        KafkaMetricSampleAggregatorMetadata aggregatorMetadata =
            new KafkaMetricSampleAggregatorMetadata(currentGeneration(), maxAllowedNumImputations());
        aggregatorMetadata.update(currentGeneration(), clusterAndGeneration.generation(),
                                  super.windowAggregationResults(-1, Long.MAX_VALUE, allPartitions(clusterAndGeneration.cluster())));
        _aggregatorMetadata = aggregatorMetadata;
      }
    }
  }

  /**
   * Get the aggregator metadata of the given list of WindowAggregationResults. Update the cached aggregator metadata
   * if needed.
   *
   * @param clusterAndGeneration the cluster information.
   * @param windowResults the list of window aggregation result to get metadata from.
   * @return the aggregator metadata of the given list of WindowAggregationResults
   */
  private KafkaMetricSampleAggregatorMetadata updateAggregatorMetadataIfNeeded(MetadataClient.ClusterAndGeneration clusterAndGeneration,
                                                                              List<WindowAggregationResult<TopicPartition>> windowResults) {
    synchronized (this) {
      long maxGenerationInList = maxGeneration(windowResults);
      if (!_aggregatorMetadata.generationsMatch(maxGenerationInList, clusterAndGeneration.generation())) {
        KafkaMetricSampleAggregatorMetadata aggregatorMetadata =
            new KafkaMetricSampleAggregatorMetadata(currentGeneration(), maxAllowedNumImputations());
        aggregatorMetadata.update(maxGenerationInList, clusterAndGeneration.generation(), windowResults);
        if (aggregatorMetadata.compareGeneration(_aggregatorMetadata) >= 0) {
          _aggregatorMetadata = aggregatorMetadata;
        }
        return aggregatorMetadata;
      } else {
        return _aggregatorMetadata;
      }
    }
  }

  private double maxAllowedNumImputations() {
    return maxAllowedNumImputations(_numSnapshotWindows);
  }
  
  private double maxAllowedNumImputations(int numSnapshotWindows) {
    return Math.max(1, numSnapshotWindows * MAX_SNAPSHOT_PERCENT_WITH_FLAWS);
  }

  private long maxGeneration(List<WindowAggregationResult<TopicPartition>> windowResults) {
    long generation = -1L;
    for (WindowAggregationResult<TopicPartition> windowResult : windowResults) {
      generation = Long.max(generation, windowResult.generation());
    }
    return generation;
  }

  private void addPartitionToResult(MetricSampleAggregationResult result,
                                    List<WindowAggregationResult<TopicPartition>> windowResults,
                                    TopicPartition tp,
                                    ModelCompletenessRequirements requirements,
                                    Set<String> topicsWithTooManyImputations,
                                    Set<String> validTopics) {
    boolean includeAllTopics = requirements.includeAllTopics();
    Snapshot[] snapshots = new Snapshot[windowResults.size()];
    int i = 0;
    for (WindowAggregationResult<TopicPartition> windowResult : windowResults) {
      snapshots[i++] = windowResult.snapshots().get(tp);
      SnapshotAndImputation.Imputation imputation = windowResult.entityWithImputations().get(tp);
      if (imputation != null) {
        result.recordPartitionWithSampleFlaw(tp, windowResult.window(), imputation);
      }
    }
    // Only record sample flaw but not add the snapshot if it should not be included.
    boolean shouldInclude = 
        includeAllTopics || (validTopics.contains(tp.topic()) && !topicsWithTooManyImputations.contains(tp.topic())); 
    if (shouldInclude) {
      result.addPartitionSnapshots(tp, snapshots);
    }
  }

  private Set<TopicPartition> allPartitions(Cluster cluster) {
    Set<TopicPartition> allPartitions = new HashSet<>();
    for (Node node : cluster.nodes()) {
      for (PartitionInfo partitionInfo : cluster.partitionsForNode(node.id())) {
        TopicPartition tp = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());
        allPartitions.add(_identityEntityMap.computeIfAbsent(tp, k -> tp));
      }
    }
    return allPartitions;
  }

  /**
   * This is a simple sanity check on the sample data. We only verify that
   * <p>
   * 1. the broker of the sampled data is from the broker who holds the leader replica. If it is not, we simply
   * discard the data because leader migration may have occurred so the metrics on the old data might not be
   * accurate anymore.
   * <p>
   * 2. The sample contains metric for all the resources.
   *
   * @param sample the sample to do the sanity check.
   * @param leaderValidation whether skip the leader validation or not.
   * @return <tt>true</tt> if the sample is valid.
   */
  private boolean isValidSample(PartitionMetricSample sample, boolean leaderValidation) {
    boolean validLeader = true;
    if (leaderValidation) {
      Node leader = _metadata.fetch().leaderFor(sample.entity());
      validLeader = (leader != null) && (sample.brokerId() == leader.id());
      if (!validLeader) {
        LOG.warn("The metric sample is discarded due to invalid leader. Current leader {}, Sample: {}", leader, sample);
      }
    }
    boolean completeMetrics = sample.numMetrics() == Resource.values().length;
    if (!completeMetrics) {
      LOG.warn("The metric sample is discarded due to missing metrics. Sample: {}", sample);
    }
    return validLeader && completeMetrics;
  }

}
