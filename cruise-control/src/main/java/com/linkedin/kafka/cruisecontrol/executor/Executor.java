/*
 * Copyright 2017 LinkedIn Corp. Licensed under the BSD 2-Clause License (the "License"). See License in the project root for license information.
 */

package com.linkedin.kafka.cruisecontrol.executor;

import com.codahale.metrics.MetricRegistry;
import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;
import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfig;
import com.linkedin.kafka.cruisecontrol.common.KafkaCruiseControlThreadFactory;
import com.linkedin.kafka.cruisecontrol.common.MetadataClient;
import com.linkedin.kafka.cruisecontrol.monitor.LoadMonitor;
import java.util.ArrayList;
import java.util.Set;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.locks.ReentrantLock;
import kafka.utils.ZkUtils;
import org.apache.kafka.clients.Metadata;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.Node;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.utils.Time;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Collection;
import java.util.List;

import static com.linkedin.kafka.cruisecontrol.executor.ExecutionTask.State.ABORTED;
import static com.linkedin.kafka.cruisecontrol.executor.ExecutionTask.State.ABORTING;
import static com.linkedin.kafka.cruisecontrol.executor.ExecutionTask.State.DEAD;
import static com.linkedin.kafka.cruisecontrol.executor.ExecutionTask.State.IN_PROGRESS;


/**
 * Executor for Kafka GoalOptimizer.
 * <p>
 * The executor class is responsible for talking to the Kafka cluster to execute the rebalance proposals.
 *
 * The executor is thread safe.
 */
public class Executor {
  private static final Logger LOG = LoggerFactory.getLogger(Executor.class);
  // The execution progress is controlled by the ExecutionTaskManager.
  private final ExecutionTaskManager _executionTaskManager;
  private final MetadataClient _metadataClient;
  private final long _statusCheckingIntervalMs;
  private final ExecutorService _proposalExecutor;
  private final String _zkConnect;

  // Some state for external service to query
  private ExecutorState.State _state;
  private final AtomicBoolean _stopRequested;
  private volatile int _numFinishedPartitionMovements;
  private volatile long _finishedDataMovementInMB;
  private final ReentrantLock _stateLock;

  /**
   * The executor class that execute the proposals generated by optimizer.
   *
   * @param config The configurations for Cruise Control.
   */
  public Executor(KafkaCruiseControlConfig config, Time time, MetricRegistry dropwizardMetricRegistry) {
    _executionTaskManager =
        new ExecutionTaskManager(config.getInt(KafkaCruiseControlConfig.NUM_CONCURRENT_PARTITION_MOVEMENTS_PER_BROKER_CONFIG),
                                 config.getInt(KafkaCruiseControlConfig.NUM_CONCURRENT_LEADER_MOVEMENTS_CONFIG),
                                 dropwizardMetricRegistry);
    _zkConnect = config.getString(KafkaCruiseControlConfig.ZOOKEEPER_CONNECT_CONFIG);
    _metadataClient = new MetadataClient(config, new Metadata(), -1L, time);
    _statusCheckingIntervalMs = config.getLong(KafkaCruiseControlConfig.EXECUTION_PROGRESS_CHECK_INTERVAL_MS_CONFIG);
    _proposalExecutor =
        Executors.newSingleThreadExecutor(new KafkaCruiseControlThreadFactory("ProposalExecutor", false, LOG));
    _state = ExecutorState.State.NO_TASK_IN_PROGRESS;
    _stopRequested = new AtomicBoolean(false);
    _stateLock = new ReentrantLock();
  }

  /**
   * Check whether the executor is executing a set of proposals.
   */
  public ExecutorState state() {
    _stateLock.lock();
    try {
      switch (_state) {
        case NO_TASK_IN_PROGRESS:
          return ExecutorState.noTaskInProgress();
        case EXECUTION_STARTED:
          return ExecutorState.executionStarted();
        case LEADER_MOVEMENT_TASK_IN_PROGRESS:
          return ExecutorState.leaderMovementInProgress();
        case REPLICA_MOVEMENT_TASK_IN_PROGRESS:
          ExecutionTaskManager.ExecutionState executionState = _executionTaskManager.getExecutionState();
          return ExecutorState.replicaMovementInProgress(_numFinishedPartitionMovements, executionState, _finishedDataMovementInMB);
        case STOPPING_EXECUTION:
          executionState = _executionTaskManager.getExecutionState();
          return ExecutorState.stopping(_numFinishedPartitionMovements, executionState, _finishedDataMovementInMB);
        default:
          throw new IllegalStateException("Should never be here!");
      }
    } finally {
      _stateLock.unlock();
    }
  }

  /**
   * (1) Add the given execution proposals.
   * (2) Start execution.
   *
   * @param proposals Proposals to be executed.
   * @param unthrottledBrokers Brokers that are not throttled in terms of the number of in/out replica movements.
   * @param loadMonitor Load monitor.
   */
  public void executeProposals(Collection<ExecutionProposal> proposals,
                               Collection<Integer> unthrottledBrokers,
                               LoadMonitor loadMonitor) {
    _stateLock.lock();
    try {
      addExecutionProposals(proposals, unthrottledBrokers);
      startExecution(loadMonitor);
    } finally {
      _stateLock.unlock();
    }
  }

  /**
   * Add the given execution proposals.
   */
  private void addExecutionProposals(Collection<ExecutionProposal> proposals, Collection<Integer> unthrottledBrokers) {
    if (_state != ExecutorState.State.NO_TASK_IN_PROGRESS) {
      throw new IllegalStateException("Cannot add new proposals while the execution is in progress.");
    }
    _executionTaskManager.addBalancingProposals(proposals, unthrottledBrokers);
  }

  /**
   * Pause the load monitor and kick off the execution.
   *
   * @param loadMonitor Load monitor.
   */
  private void startExecution(LoadMonitor loadMonitor) {
    if (loadMonitor == null) {
      throw new IllegalArgumentException("Load monitor cannot be null.");
    }
    // Pause the metric sampling to avoid the loss of accuracy during execution.
    loadMonitor.pauseMetricSampling();
    ZkUtils zkUtils = ZkUtils.apply(_zkConnect, 30000, 30000, false);
    try {
      if (!ExecutorUtils.partitionsBeingReassigned(zkUtils).isEmpty()) {
        _executionTaskManager.clear();
        throw new IllegalStateException("There are ongoing partition reassignments.");
      }
      if (_state == ExecutorState.State.NO_TASK_IN_PROGRESS) {
        _state = ExecutorState.State.EXECUTION_STARTED;
        _proposalExecutor.submit(new ProposalExecutionRunnable(loadMonitor));
      } else {
        throw new IllegalStateException("Cannot execute proposals because the executor is in " + _state + " state.");
      }
    } finally {
      KafkaCruiseControlUtils.closeZkUtilsWithTimeout(zkUtils, 10000);
    }
  }

  public void stopExecution() {
    _stateLock.lock();
    if (_state != ExecutorState.State.NO_TASK_IN_PROGRESS) {
      _state = ExecutorState.State.STOPPING_EXECUTION;
      _stateLock.unlock();
      _stopRequested.set(true);
    } else {
      _stateLock.unlock();
    }
  }

  /**
   * Shutdown the executor.
   */
  public void shutdown() {
    _stateLock.lock();
    try {
      LOG.info("Shutting down executor.");
      if (_state != ExecutorState.State.NO_TASK_IN_PROGRESS) {
        LOG.warn("Shutdown executor may take long because execution is still in progress.");
      }
      _proposalExecutor.shutdown();

      try {
        _proposalExecutor.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);
      } catch (InterruptedException e) {
        LOG.warn("Interrupted while waiting for anomaly detector to shutdown.");
      }
      LOG.info("Executor shutdown completed.");
    } finally {
      _stateLock.unlock();
    }
  }

  /**
   * This class is thread safe.
   *
   * Note that once the thread for {@link ProposalExecutionRunnable} is submitted for running, the variable
   * _executionTaskManager can only be written within this inner class, but not from the outer Executor class.
   */
  private class ProposalExecutionRunnable implements Runnable {
    private final LoadMonitor _loadMonitor;
    private ZkUtils _zkUtils;

    ProposalExecutionRunnable(LoadMonitor loadMonitor) {
      _loadMonitor = loadMonitor;
    }

    public void run() {
      LOG.info("Starting executing balancing proposals.");
      execute();
      LOG.info("Execution finished.");
    }

    private void maybeMove(ExecutorState.State expectedState, ExecutorState.State nextState, Runnable moveRunnable) {
      boolean isExpectedState = false;
      _stateLock.lock();
      if (_state == expectedState) {
        _state = nextState;
        isExpectedState = true;
      }
      _stateLock.unlock();

      if (isExpectedState) {
        moveRunnable.run();
      }
    }

    /**
     * Start the actual execution of the proposals in order: First move replicas, then transfer leadership.
     */
    private void execute() {
      _zkUtils = ZkUtils.apply(_zkConnect, 30000, 30000, false);
      try {
        // 1. Move replicas if possible.
        maybeMove(ExecutorState.State.EXECUTION_STARTED,
                  ExecutorState.State.REPLICA_MOVEMENT_TASK_IN_PROGRESS,
                  this::moveReplicas);

        // 2. Transfer leadership if possible.
        maybeMove(ExecutorState.State.REPLICA_MOVEMENT_TASK_IN_PROGRESS,
                  ExecutorState.State.LEADER_MOVEMENT_TASK_IN_PROGRESS,
                  this::moveLeaderships);

      } catch (Throwable t) {
        LOG.error("Executor got exception during execution", t);
      } finally {
        _loadMonitor.resumeMetricSampling();
        _stopRequested.set(false);
        _executionTaskManager.clear();
        KafkaCruiseControlUtils.closeZkUtilsWithTimeout(_zkUtils, 10000);
        _stateLock.lock();
        _state = ExecutorState.State.NO_TASK_IN_PROGRESS;
        _stateLock.unlock();
      }
    }

    /**
     * Get the in execution data to move in MB.
     * @param inExecutionTasks Tasks that are either in progress or aborting.
     * @return the in execution data to move in MB.
     */
    private long inExecutionDataToMoveInMB(Set<ExecutionTask> inExecutionTasks) {
      return inExecutionTasks.stream().mapToLong(task -> task.proposal().dataToMoveInMB()).sum();
    }

    private void moveReplicas() {
      int numTotalPartitionMovements = _executionTaskManager.remainingPartitionMovements().size();
      long totalDataToMoveInMB = _executionTaskManager.remainingDataToMoveInMB();
      LOG.info("Starting {} partition movements.", numTotalPartitionMovements);

      int partitionsToMove = numTotalPartitionMovements;
      // Exhaust all the pending partition movements.
      while ((partitionsToMove > 0 || !_executionTaskManager.inExecutionTasks().isEmpty()) && !_stopRequested.get()) {
        // Get tasks to execute.
        List<ExecutionTask> tasksToExecute = _executionTaskManager.getReplicaMovementTasks();
        LOG.info("Executor will execute {} task(s)", tasksToExecute.size());

        if (!tasksToExecute.isEmpty()) {
          // Execute the tasks.
          _executionTaskManager.markTasksInProgress(tasksToExecute);
          ExecutorUtils.executeReplicaReassignmentTasks(_zkUtils, tasksToExecute);
        }
        // Wait indefinitely for partition movements to finish.
        waitForExecutionTaskToFinish();
        partitionsToMove = _executionTaskManager.remainingPartitionMovements().size();
        long dataToMove = _executionTaskManager.remainingDataToMoveInMB();
        Set<ExecutionTask> inExecutionTasks = _executionTaskManager.inExecutionTasks();
        _numFinishedPartitionMovements = numTotalPartitionMovements - partitionsToMove - inExecutionTasks.size();
        _finishedDataMovementInMB = totalDataToMoveInMB - dataToMove - inExecutionDataToMoveInMB(inExecutionTasks);
        LOG.info("{}/{} ({}%) partition movements completed. {}/{} ({}%) MB have been moved.",
                 _numFinishedPartitionMovements, numTotalPartitionMovements,
                 String.format(java.util.Locale.US, "%.2f",
                               _numFinishedPartitionMovements * 100.0 / numTotalPartitionMovements),
                 _finishedDataMovementInMB, totalDataToMoveInMB,
                 totalDataToMoveInMB == 0 ? 100 : String.format(java.util.Locale.US, "%.2f",
                                                                (_finishedDataMovementInMB * 100.0) / totalDataToMoveInMB));
      }
      // After the partition movement finishes, wait for the controller to clean the reassignment zkPath. This also
      // ensures a clean stop when the execution is stopped in the middle.
      Set<ExecutionTask> inExecutionTasks = _executionTaskManager.inExecutionTasks();
      while (!inExecutionTasks.isEmpty()) {
        LOG.info("Waiting for {} tasks moving {} MB to finish: {}",
                 inExecutionTasks.size(), inExecutionDataToMoveInMB(inExecutionTasks), inExecutionTasks);
        waitForExecutionTaskToFinish();
        inExecutionTasks = _executionTaskManager.inExecutionTasks();
      }
      if (_executionTaskManager.inProgressTasks().isEmpty()) {
        LOG.info("Partition movements finished.");
      } else if (_stopRequested.get()) {
        ExecutionTaskManager.ExecutionState executionState = _executionTaskManager.getExecutionState();
        LOG.info("Partition movements stopped. {} in-progress, {} pending, {} aborting, {} aborted, {} dead, "
                 + "{} remaining data to move.",
                 executionState.inProgressTasks().size(),
                 executionState.remainingPartitionMovements().size(),
                 executionState.abortingTasks(),
                 executionState.abortedTasks().size(),
                 executionState.deadTasks().size(),
                 executionState.remainingDataToMoveInMB());
      }
    }

    private void moveLeaderships() {
      int numTotalLeaderMovements = _executionTaskManager.remainingLeaderMovements().size();
      LOG.info("Starting {} leader movements.", numTotalLeaderMovements);
      int leaderMoved = 0;
      while (!_executionTaskManager.remainingLeaderMovements().isEmpty() && !_stopRequested.get()) {
        leaderMoved += moveLeadersInBatch();
        LOG.info("{}/{} ({}%) leader movements completed.", leaderMoved, numTotalLeaderMovements,
                 leaderMoved * 100 / numTotalLeaderMovements);
      }
      LOG.info("Leader movements finished.");
    }

    private int moveLeadersInBatch() {
      List<ExecutionTask> leaderMovementTasks = _executionTaskManager.getLeaderMovementTasks();
      int numLeadersToMove = leaderMovementTasks.size();
      LOG.debug("Executing {} leader movements in a batch.", numLeadersToMove);
      // Execute the leader movements.
      if (!leaderMovementTasks.isEmpty() && !_stopRequested.get()) {
        // Mark leader movements in progress.
        _executionTaskManager.markTasksInProgress(leaderMovementTasks);

        // Run preferred leader election.
        ExecutorUtils.executePreferredLeaderElection(_zkUtils, leaderMovementTasks);
        LOG.trace("Waiting for leader movement batch to finish.");
        while (!_executionTaskManager.inProgressTasks().isEmpty() && !_stopRequested.get()) {
          waitForExecutionTaskToFinish();
        }
      }
      return numLeadersToMove;
    }

    /**
     * This method periodically check zookeeper to see if the partition reassignment has finished or not.
     */
    private void waitForExecutionTaskToFinish() {
      List<ExecutionTask> finishedTasks = new ArrayList<>();
      do {
        Cluster cluster = _metadataClient.refreshMetadata().cluster();
        LOG.debug("Tasks in execution: {}", _executionTaskManager.inExecutionTasks());
        List<ExecutionTask> deadOrAbortingTasks = new ArrayList<>();
        for (ExecutionTask task : _executionTaskManager.inExecutionTasks()) {
          TopicPartition tp = task.proposal().topicPartition();
          if (cluster.partition(tp) == null) {
            // Handle topic deletion during the execution.
            LOG.debug("Task {} is marked as finished because the topic has been deleted", task);
            finishedTasks.add(task);
            _executionTaskManager.markTaskAborting(task);
            _executionTaskManager.markTaskDone(task);
          } else if (isTaskDone(cluster, tp, task)) {
            // Check to see if the task is done.
            finishedTasks.add(task);
            _executionTaskManager.markTaskDone(task);
          } else if (maybeMarkTaskAsDeadOrAborting(cluster, task)) {
            // Only add the dead or aborted tasks to execute if it is not a leadership movement.
            if (task.type() != ExecutionTask.TaskType.LEADER_ACTION) {
              deadOrAbortingTasks.add(task);
            }
            // A dead or aborted task is considered as finished.
            if (task.state() == DEAD || task.state() == ABORTED) {
              finishedTasks.add(task);
            }
          }
        }
        // TODO: Execute the dead or aborted tasks.
        if (!deadOrAbortingTasks.isEmpty()) {
          // TODO: re-enable this rollback action when KAFKA-6304 is available.
          // ExecutorUtils.executeReplicaReassignmentTasks(_zkUtils, deadOrAbortingTasks);
          if (!_stopRequested.get()) {
            // If there is task aborted or dead, we stop the execution.
            stopExecution();
          }
        }

        // If there is no finished tasks, we need to check if anything is blocked.
        if (finishedTasks.isEmpty()) {
          maybeReexecuteTasks();
          try {
            Thread.sleep(_statusCheckingIntervalMs);
          } catch (InterruptedException e) {
            // let it go
          }
        }
      } while (!_executionTaskManager.inExecutionTasks().isEmpty() && finishedTasks.size() == 0);
      // Some tasks have finished, remove them from in progress task map.
      LOG.info("Completed tasks: {}", finishedTasks);
    }

    /**
     * Check if a task is done.
     */
    private boolean isTaskDone(Cluster cluster, TopicPartition tp, ExecutionTask task) {
      if (task.type() == ExecutionTask.TaskType.REPLICA_ACTION) {
        return isReplicaActionDone(tp, task);
      } else {
        return isLeadershipMovementDone(cluster, tp, task);
      }
    }

    /**
     * For a replica action, the completion depends on the task state:
     * IN_PROGRESS: when the current replica list is the same as the new replica list.
     * ABORTING: done when the current replica list is the same as the old replica list. Due to race condition,
     *           we also consider it done if the current replica list is the same as the new replica list.
     * DEAD: always considered as done because we neither move forward or rollback.
     *
     * There should be no other task state seen here.
     */
    private boolean isReplicaActionDone(TopicPartition tp, ExecutionTask task) {
      // TODO: switch to use cluster instead of zkUtils once the broker is upgraded to 0.11.0 and above.
      List<Integer> currentReplicas = ExecutorUtils.currentReplicasForPartition(_zkUtils, tp);
      switch (task.state()) {
        case IN_PROGRESS:
          return currentReplicas.equals(task.proposal().newReplicas());
        case ABORTING:
          LOG.trace("Checking replica action {}, current replicas: {}", task, currentReplicas);
          // There could be a race condition that when we abort a task, it is already completed.
          // in that case, we treat it as aborted as well.
          return currentReplicas.equals(task.proposal().oldReplicas())
              || currentReplicas.equals(task.proposal().newReplicas());
        case DEAD:
          return true;
        default:
          throw new IllegalStateException("Should never be here. State " + task.state());
      }
    }

    /**
     * The completeness of leadership movement depends on the task state:
     * IN_PROGRESS: done when the leader becomes the destination.
     * ABORTING or DEAD: always considered as done the destination cannot become leader anymore.
     *
     * There should be no other task state seen here.
     */
    private boolean isLeadershipMovementDone(Cluster cluster, TopicPartition tp, ExecutionTask task) {
      Node leader = cluster.leaderFor(tp);
      switch (task.state()) {
        case IN_PROGRESS:
          return leader != null && leader.id() == task.proposal().newReplicas().get(0);
        case ABORTING:
        case DEAD:
          return true;
        default:
          throw new IllegalStateException("Should never be here.");
      }

    }

    /**
     * TODO: this method is not used now. It should be used after Kafka server is at 0.11.0+
     * Get the current replica set for a partition.
     */
    private List<Integer> currentReplicas(Cluster cluster, TopicPartition tp) {
      List<Integer> currentReplicas = new ArrayList<>();
      for (Node node : cluster.partition(tp).replicas()) {
        currentReplicas.add(node.id());
      }
      return currentReplicas;
    }

    /**
     * Mark the task as aborting or dead if needed.
     *
     * Ideally, the task should be marked as:
     * 1. ABORTING: when the execution is stopped by the users.
     * 2. ABORTING: When the destination broker is dead so the task cannot make progress, but the source broker is
     *              still alive.
     * 3. DEAD: when any replica in the new replica list is dead.
     *
     * Currently KafkaController does not support updates on the partitions that is being reassigned. (KAFKA-6034)
     * Therefore once a proposals is written to ZK, we cannot revoke it. So the actual behavior we are using is to
     * set the task state to:
     * 1. IN_PROGRESS: when the execution is stopped by the users. i.e. do nothing but let the task finish normally.
     * 2. DEAD: when the destination broker is dead. i.e. do not block on the execution.
     *
     * @param cluster the kafka cluster
     * @param task the task to check
     * @return true if the task is marked as dead or aborting, false otherwise.
     */
    private boolean maybeMarkTaskAsDeadOrAborting(Cluster cluster, ExecutionTask task) {
      // Only check tasks with IN_PROGRESS or ABORTING state.
      if (task.state() == IN_PROGRESS || task.state() == ABORTING) {
        if (task.type() == ExecutionTask.TaskType.LEADER_ACTION
            && cluster.nodeById(task.proposal().newReplicas().get(0)) == null) {
          _executionTaskManager.markTaskDead(task);
          LOG.warn("Killing execution for task {} because the target leader is down", task);
          return true;
        } else {
          for (int broker : task.proposal().newReplicas()) {
            if (cluster.nodeById(broker) == null) {
              _executionTaskManager.markTaskDead(task);
              LOG.warn("Killing execution for task {} because the new replica {} is down.", task, broker);
              return true;
            }
          }
        }
      }
      return false;
    }

    /**
     * Due to the race condition between the controller and Cruise Control, some of the submitted tasks may be
     * deleted by controller without being executed. We will resubmit those tasks in that case.
     */
    private void maybeReexecuteTasks() {
      boolean shouldReexecuteTasks = !_executionTaskManager.inExecutionTasks().isEmpty() &&
          ExecutorUtils.partitionsBeingReassigned(_zkUtils).isEmpty();
      if (shouldReexecuteTasks) {
        LOG.info("Reexecuting tasks {}", _executionTaskManager.inExecutionTasks());
        List<ExecutionTask> replicaActionsToReexecute = new ArrayList<>();
        for (ExecutionTask executionTask : _executionTaskManager.inExecutionTasks()) {
          if (executionTask.type() == ExecutionTask.TaskType.REPLICA_ACTION) {
            replicaActionsToReexecute.add(executionTask);
          }
        }
        ExecutorUtils.executeReplicaReassignmentTasks(_zkUtils, replicaActionsToReexecute);
      }
    }
  }
}
