/*
 * Copyright 2017 LinkedIn Corp. Licensed under the BSD 2-Clause License (the "License"). See License in the project root for license information.
 */

package com.linkedin.kafka.cruisecontrol.executor;

import com.codahale.metrics.MetricRegistry;
import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;
import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfig;
import com.linkedin.kafka.cruisecontrol.common.KafkaCruiseControlThreadFactory;
import com.linkedin.kafka.cruisecontrol.common.MetadataClient;
import com.linkedin.kafka.cruisecontrol.monitor.LoadMonitor;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Set;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import kafka.utils.ZkUtils;
import org.apache.kafka.clients.Metadata;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.Node;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.utils.Time;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Collection;
import java.util.List;

import static com.linkedin.kafka.cruisecontrol.executor.ExecutionTask.State.ABORTED;
import static com.linkedin.kafka.cruisecontrol.executor.ExecutionTask.State.ABORTING;
import static com.linkedin.kafka.cruisecontrol.executor.ExecutionTask.State.DEAD;
import static com.linkedin.kafka.cruisecontrol.executor.ExecutionTask.State.IN_PROGRESS;


/**
 * Executor for Kafka GoalOptimizer.
 * <p>
 * The executor class is responsible for talking to the Kafka cluster to execute the rebalance proposals.
 *
 * The executor is thread-safe.
 */
public class Executor {
  private static final Logger LOG = LoggerFactory.getLogger(Executor.class);
  // The maximum time to wait for a leader movement to finish. A leader movement will be marked as failed if
  // it takes longer than this time to finish.
  private static final long LEADER_ACTION_TIMEOUT_MS = 180000L;
  // The execution progress is controlled by the ExecutionTaskManager.
  private final ExecutionTaskManager _executionTaskManager;
  private final MetadataClient _metadataClient;
  private final long _statusCheckingIntervalMs;
  private final ExecutorService _proposalExecutor;
  private final String _zkConnect;
  private final static int ZK_SESSION_TIMEOUT = 30000;
  private final static int ZK_CONNECTION_TIMEOUT = 30000;
  private final static boolean IS_ZK_SECURITY_ENABLED = false;
  private final static long ZK_UTILS_CLOSE_TIMEOUT_MS = 10000;

  private final static long METADATA_REFRESH_BACKOFF = 100L;
  private final static long METADATA_EXPIRY_MS = Long.MAX_VALUE;

  // Some state for external service to query
  private final AtomicBoolean _stopRequested;
  private final Time _time;
  private volatile boolean _hasOngoingExecution;
  private volatile int _numFinishedPartitionMovements;
  private volatile int _numFinishedLeadershipMovements;
  private volatile long _finishedDataMovementInMB;
  private volatile ExecutorState _executorState;

  /**
   * The executor class that execute the proposals generated by optimizer.
   *
   * @param config The configurations for Cruise Control.
   */
  public Executor(KafkaCruiseControlConfig config, Time time, MetricRegistry dropwizardMetricRegistry) {
    _time = time;
    _executionTaskManager =
        new ExecutionTaskManager(config.getInt(KafkaCruiseControlConfig.NUM_CONCURRENT_PARTITION_MOVEMENTS_PER_BROKER_CONFIG),
                                 config.getInt(KafkaCruiseControlConfig.NUM_CONCURRENT_LEADER_MOVEMENTS_CONFIG),
                                 dropwizardMetricRegistry,
                                 time);
    _zkConnect = config.getString(KafkaCruiseControlConfig.ZOOKEEPER_CONNECT_CONFIG);
    _metadataClient = new MetadataClient(config,
                                         new Metadata(METADATA_REFRESH_BACKOFF, METADATA_EXPIRY_MS, false),
                                         -1L,
                                         time);
    _statusCheckingIntervalMs = config.getLong(KafkaCruiseControlConfig.EXECUTION_PROGRESS_CHECK_INTERVAL_MS_CONFIG);
    _proposalExecutor =
        Executors.newSingleThreadExecutor(new KafkaCruiseControlThreadFactory("ProposalExecutor", false, LOG));
    _executorState = ExecutorState.noTaskInProgress();
    _stopRequested = new AtomicBoolean(false);
    _hasOngoingExecution = false;
  }

  /**
   * The executor class that execute the proposals generated by optimizer.
   * Package private for unit test.
   *
   * @param config The configurations for Cruise Control.
   */
  Executor(KafkaCruiseControlConfig config,
           Time time,
           MetricRegistry dropwizardMetricRegistry,
           MetadataClient metadataClient) {
    _time = time;
    _executionTaskManager =
        new ExecutionTaskManager(config.getInt(KafkaCruiseControlConfig.NUM_CONCURRENT_PARTITION_MOVEMENTS_PER_BROKER_CONFIG),
            config.getInt(KafkaCruiseControlConfig.NUM_CONCURRENT_LEADER_MOVEMENTS_CONFIG),
            dropwizardMetricRegistry,
            time);
    _zkConnect = config.getString(KafkaCruiseControlConfig.ZOOKEEPER_CONNECT_CONFIG);
    _metadataClient = metadataClient;
    _statusCheckingIntervalMs = config.getLong(KafkaCruiseControlConfig.EXECUTION_PROGRESS_CHECK_INTERVAL_MS_CONFIG);
    _proposalExecutor =
        Executors.newSingleThreadExecutor(new KafkaCruiseControlThreadFactory("ProposalExecutor", false, LOG));
    _executorState = ExecutorState.noTaskInProgress();
    _stopRequested = new AtomicBoolean(false);
    _hasOngoingExecution = false;
  }

  /**
   * Check whether the executor is executing a set of proposals.
   */
  public ExecutorState state() {
    return _executorState;
  }

  /**
   * (1) Add the given execution proposals.
   * (2) Start execution.
   *
   * @param proposals Proposals to be executed.
   * @param unthrottledBrokers Brokers that are not throttled in terms of the number of in/out replica movements.
   * @param loadMonitor Load monitor.
   */
  public synchronized void executeProposals(Collection<ExecutionProposal> proposals,
                                            Collection<Integer> unthrottledBrokers,
                                            LoadMonitor loadMonitor) {
    if (_hasOngoingExecution) {
      throw new IllegalStateException("Cannot execute new proposals while there is an ongoing execution.");
    }

    if (loadMonitor == null) {
      throw new IllegalArgumentException("Load monitor cannot be null.");
    }
    _executionTaskManager.addExecutionProposals(proposals, unthrottledBrokers, _metadataClient.refreshMetadata().cluster());
    startExecution(loadMonitor);
  }

  /**
   * Pause the load monitor and kick off the execution.
   *
   * @param loadMonitor Load monitor.
   */
  private void startExecution(LoadMonitor loadMonitor) {
    ZkUtils zkUtils = ZkUtils.apply(_zkConnect, ZK_SESSION_TIMEOUT, ZK_CONNECTION_TIMEOUT, IS_ZK_SECURITY_ENABLED);
    try {
      if (!ExecutorUtils.partitionsBeingReassigned(zkUtils).isEmpty()) {
        _executionTaskManager.clear();
        // Note that in case there is an ongoing partition reassignment, we do not unpause metric sampling.
        throw new IllegalStateException("There are ongoing partition reassignments.");
      }
      _hasOngoingExecution = true;
      _stopRequested.set(false);
      _proposalExecutor.submit(new ProposalExecutionRunnable(loadMonitor));
    } finally {
      KafkaCruiseControlUtils.closeZkUtilsWithTimeout(zkUtils, ZK_UTILS_CLOSE_TIMEOUT_MS);
    }
  }

  public synchronized void stopExecution() {
    _stopRequested.set(true);
  }

  /**
   * Shutdown the executor.
   */
  public synchronized void shutdown() {
    LOG.info("Shutting down executor.");
    if (_hasOngoingExecution) {
      LOG.warn("Shutdown executor may take long because execution is still in progress.");
    }
    _proposalExecutor.shutdown();

    try {
      _proposalExecutor.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);
    } catch (InterruptedException e) {
      LOG.warn("Interrupted while waiting for anomaly detector to shutdown.");
    }
    _metadataClient.close();
    LOG.info("Executor shutdown completed.");
  }

  /**
   * package private for unit test.
   */
  boolean hasOngoingExecution() {
    return _hasOngoingExecution;
  }

  /**
   * This class is thread safe.
   *
   * Note that once the thread for {@link ProposalExecutionRunnable} is submitted for running, the variable
   * _executionTaskManager can only be written within this inner class, but not from the outer Executor class.
   */
  private class ProposalExecutionRunnable implements Runnable {
    private final LoadMonitor _loadMonitor;
    private ZkUtils _zkUtils;
    private ExecutorState.State _state;

    ProposalExecutionRunnable(LoadMonitor loadMonitor) {
      _loadMonitor = loadMonitor;
      _state = ExecutorState.State.NO_TASK_IN_PROGRESS;
    }

    public void run() {
      LOG.info("Starting executing balancing proposals.");
      execute();
      LOG.info("Execution finished.");
    }

    /**
     * Start the actual execution of the proposals in order: First move replicas, then transfer leadership.
     */
    private void execute() {
      _state = ExecutorState.State.EXECUTION_STARTED;
      _executorState = ExecutorState.executionStarted();
      _zkUtils = ZkUtils.apply(_zkConnect, ZK_SESSION_TIMEOUT, ZK_CONNECTION_TIMEOUT, IS_ZK_SECURITY_ENABLED);
      // Pause the metric sampling to avoid the loss of accuracy during execution.
      _loadMonitor.pauseMetricSampling();
      try {
        // 1. Move replicas if possible.
        if (_state == ExecutorState.State.EXECUTION_STARTED) {
          _state = ExecutorState.State.REPLICA_MOVEMENT_TASK_IN_PROGRESS;
          // The _executorState might be inconsistent with _state if the user checks it between the two assignments.
          _executorState = ExecutorState.replicaMovementInProgress(_numFinishedPartitionMovements,
                                                                   _executionTaskManager.getExecutionTasksSummary(),
                                                                   _finishedDataMovementInMB);
          moveReplicas();
          updateOngoingExecutionState();
        }
        // 2. Transfer leadership if possible.
        if (_state == ExecutorState.State.REPLICA_MOVEMENT_TASK_IN_PROGRESS) {
          _state = ExecutorState.State.LEADER_MOVEMENT_TASK_IN_PROGRESS;
          // The _executorState might be inconsistent with _state if the user checks it between the two assignments.
          _executorState = ExecutorState.leaderMovementInProgress(_numFinishedLeadershipMovements,
                                                                  _executionTaskManager.getExecutionTasksSummary());
          moveLeaderships();
          updateOngoingExecutionState();
        }

      } catch (Throwable t) {
        LOG.error("Executor got exception during execution", t);
      } finally {
        _loadMonitor.resumeMetricSampling();
        _executionTaskManager.clear();
        KafkaCruiseControlUtils.closeZkUtilsWithTimeout(_zkUtils, ZK_UTILS_CLOSE_TIMEOUT_MS);
        _state = ExecutorState.State.NO_TASK_IN_PROGRESS;
        // The _executorState might be inconsistent with _state if the user checks it between the two assignments.
        _executorState = ExecutorState.noTaskInProgress();
        _hasOngoingExecution = false;
        _stopRequested.set(false);
      }
    }

    private void updateOngoingExecutionState() {
      if (!_stopRequested.get()) {
        switch (_state) {
          case LEADER_MOVEMENT_TASK_IN_PROGRESS:
            _executorState = ExecutorState.leaderMovementInProgress(_numFinishedLeadershipMovements,
                                                                    _executionTaskManager.getExecutionTasksSummary());
            break;
          case REPLICA_MOVEMENT_TASK_IN_PROGRESS:
            _executorState = ExecutorState.replicaMovementInProgress(_numFinishedPartitionMovements,
                                                                     _executionTaskManager.getExecutionTasksSummary(),
                                                                     _finishedDataMovementInMB);
            break;
          default:
            throw new IllegalStateException("Unexpected ongoing execution state " + _state);
        }
      } else {
        _state = ExecutorState.State.STOPPING_EXECUTION;
        _executorState = ExecutorState.stopping(_numFinishedPartitionMovements,
                                                _numFinishedLeadershipMovements,
                                                _executionTaskManager.getExecutionTasksSummary(),
                                                _finishedDataMovementInMB);
      }
    }

    private void moveReplicas() {
      int numTotalPartitionMovements = _executionTaskManager.remainingPartitionMovements().size();
      long totalDataToMoveInMB = _executionTaskManager.remainingDataToMoveInMB();
      LOG.info("Starting {} partition movements.", numTotalPartitionMovements);

      int partitionsToMove = numTotalPartitionMovements;
      // Exhaust all the pending partition movements.
      while ((partitionsToMove > 0 || !_executionTaskManager.inExecutionTasks().isEmpty()) && !_stopRequested.get()) {
        // Get tasks to execute.
        List<ExecutionTask> tasksToExecute = _executionTaskManager.getReplicaMovementTasks();
        LOG.info("Executor will execute {} task(s)", tasksToExecute.size());

        if (!tasksToExecute.isEmpty()) {
          // Execute the tasks.
          _executionTaskManager.markTasksInProgress(tasksToExecute);
          ExecutorUtils.executeReplicaReassignmentTasks(_zkUtils, tasksToExecute);
        }
        // Wait indefinitely for partition movements to finish.
        waitForExecutionTaskToFinish();
        partitionsToMove = _executionTaskManager.remainingPartitionMovements().size();
        long dataToMove = _executionTaskManager.remainingDataToMoveInMB();
        Set<ExecutionTask> inExecutionTasks = _executionTaskManager.inExecutionTasks();
        _numFinishedPartitionMovements = numTotalPartitionMovements - partitionsToMove - inExecutionTasks.size();
        _finishedDataMovementInMB = totalDataToMoveInMB - dataToMove - _executionTaskManager.inExecutionDataToMoveInMB();
        LOG.info("{}/{} ({}%) partition movements completed. {}/{} ({}%) MB have been moved.",
                 _numFinishedPartitionMovements, numTotalPartitionMovements,
                 String.format(java.util.Locale.US, "%.2f",
                               _numFinishedPartitionMovements * 100.0 / numTotalPartitionMovements),
                 _finishedDataMovementInMB, totalDataToMoveInMB,
                 totalDataToMoveInMB == 0 ? 100 : String.format(java.util.Locale.US, "%.2f",
                                                                (_finishedDataMovementInMB * 100.0) / totalDataToMoveInMB));
      }
      // After the partition movement finishes, wait for the controller to clean the reassignment zkPath. This also
      // ensures a clean stop when the execution is stopped in the middle.
      Set<ExecutionTask> inExecutionTasks = _executionTaskManager.inExecutionTasks();
      while (!inExecutionTasks.isEmpty()) {
        LOG.info("Waiting for {} tasks moving {} MB to finish: {}",
                 inExecutionTasks.size(), _executionTaskManager.inExecutionDataToMoveInMB(), inExecutionTasks);
        waitForExecutionTaskToFinish();
        inExecutionTasks = _executionTaskManager.inExecutionTasks();
      }
      if (_executionTaskManager.inProgressTasks().isEmpty()) {
        LOG.info("Partition movements finished.");
      } else if (_stopRequested.get()) {
        ExecutionTaskManager.ExecutionTasksSummary executionTasksSummary = _executionTaskManager.getExecutionTasksSummary();
        LOG.info("Partition movements stopped. {} in-progress, {} pending, {} aborting, {} aborted, {} dead, "
                 + "{} remaining data to move.",
                 executionTasksSummary.inProgressTasks().size(),
                 executionTasksSummary.remainingPartitionMovements().size(),
                 executionTasksSummary.abortingTasks(),
                 executionTasksSummary.abortedTasks().size(),
                 executionTasksSummary.deadTasks().size(),
                 executionTasksSummary.remainingDataToMoveInMB());
      }
    }

    private void moveLeaderships() {
      int numTotalLeadershipMovements = _executionTaskManager.remainingLeadershipMovements().size();
      LOG.info("Starting {} leadership movements.", numTotalLeadershipMovements);
      _numFinishedLeadershipMovements = 0;
      while (!_executionTaskManager.remainingLeadershipMovements().isEmpty() && !_stopRequested.get()) {
        updateOngoingExecutionState();
        _numFinishedLeadershipMovements += moveLeadershipInBatch();
        LOG.info("{}/{} ({}%) leadership movements completed.", _numFinishedLeadershipMovements,
                 numTotalLeadershipMovements, _numFinishedLeadershipMovements * 100 / numTotalLeadershipMovements);
      }
      LOG.info("Leadership movements finished.");
    }

    private int moveLeadershipInBatch() {
      List<ExecutionTask> leadershipMovementTasks = _executionTaskManager.getLeadershipMovementTasks();
      int numLeadershipToMove = leadershipMovementTasks.size();
      LOG.debug("Executing {} leadership movements in a batch.", numLeadershipToMove);
      // Execute the leadership movements.
      if (!leadershipMovementTasks.isEmpty() && !_stopRequested.get()) {
        // Mark leadership movements in progress.
        _executionTaskManager.markTasksInProgress(leadershipMovementTasks);

        // Run preferred leader election.
        ExecutorUtils.executePreferredLeaderElection(_zkUtils, leadershipMovementTasks);
        LOG.trace("Waiting for leadership movement batch to finish.");
        while (!_executionTaskManager.inProgressTasks().isEmpty() && !_stopRequested.get()) {
          waitForExecutionTaskToFinish();
        }
      }
      return numLeadershipToMove;
    }

    /**
     * This method periodically check zookeeper to see if the partition reassignment has finished or not.
     */
    private void waitForExecutionTaskToFinish() {
      List<ExecutionTask> finishedTasks = new ArrayList<>();
      do {
        // If there is no finished tasks, we need to check if anything is blocked.
        if (finishedTasks.isEmpty()) {
          maybeReexecuteTasks();
          try {
            Thread.sleep(_statusCheckingIntervalMs);
          } catch (InterruptedException e) {
            // let it go
          }
        }

        Cluster cluster = _metadataClient.refreshMetadata().cluster();
        LOG.debug("Tasks in execution: {}", _executionTaskManager.inExecutionTasks());
        List<ExecutionTask> deadOrAbortingTasks = new ArrayList<>();
        for (ExecutionTask task : _executionTaskManager.inExecutionTasks()) {
          TopicPartition tp = task.proposal().topicPartition();
          if (cluster.partition(tp) == null) {
            // Handle topic deletion during the execution.
            LOG.debug("Task {} is marked as finished because the topic has been deleted", task);
            finishedTasks.add(task);
            _executionTaskManager.markTaskAborting(task);
            _executionTaskManager.markTaskDone(task);
          } else if (isTaskDone(cluster, tp, task)) {
            // Check to see if the task is done.
            finishedTasks.add(task);
            _executionTaskManager.markTaskDone(task);
          } else if (maybeMarkTaskAsDeadOrAborting(cluster, task)) {
            // Only add the dead or aborted tasks to execute if it is not a leadership movement.
            if (task.type() != ExecutionTask.TaskType.LEADER_ACTION) {
              deadOrAbortingTasks.add(task);
            }
            // A dead or aborted task is considered as finished.
            if (task.state() == DEAD || task.state() == ABORTED) {
              finishedTasks.add(task);
            }
          }
        }
        // TODO: Execute the dead or aborted tasks.
        if (!deadOrAbortingTasks.isEmpty()) {
          // TODO: re-enable this rollback action when KAFKA-6304 is available.
          // ExecutorUtils.executeReplicaReassignmentTasks(_zkUtils, deadOrAbortingTasks);
          if (!_stopRequested.get()) {
            // If there is task aborted or dead, we stop the execution.
            stopExecution();
          }
        }
        updateOngoingExecutionState();
      } while (!_executionTaskManager.inExecutionTasks().isEmpty() && finishedTasks.size() == 0);
      // Some tasks have finished, remove them from in progress task map.
      LOG.info("Completed tasks: {}", finishedTasks);
    }

    /**
     * Check if a task is done.
     */
    private boolean isTaskDone(Cluster cluster, TopicPartition tp, ExecutionTask task) {
      if (task.type() == ExecutionTask.TaskType.REPLICA_ACTION) {
        return isReplicaActionDone(cluster, tp, task);
      } else {
        return isLeadershipMovementDone(cluster, tp, task);
      }
    }

    /**
     * For a replica action, the completion depends on the task state:
     * IN_PROGRESS: when the current replica list is the same as the new replica list.
     * ABORTING: done when the current replica list is the same as the old replica list. Due to race condition,
     *           we also consider it done if the current replica list is the same as the new replica list.
     * DEAD: always considered as done because we neither move forward or rollback.
     *
     * There should be no other task state seen here.
     */
    private boolean isReplicaActionDone(Cluster cluster, TopicPartition tp, ExecutionTask task) {
      LOG.trace("Current replica task: {}.", task);
      Node[] currentOrderedReplicas = cluster.partition(tp).replicas();
      switch (task.state()) {
        case IN_PROGRESS:
          return task.proposal().isCompletedSuccessfully(currentOrderedReplicas);
        case ABORTING:
          return task.proposal().isAborted(currentOrderedReplicas);
        case DEAD:
          return true;
        default:
          throw new IllegalStateException("Should never be here. State " + task.state());
      }
    }

    private boolean isInIsr(Integer leader, Cluster cluster, TopicPartition tp) {
      return Arrays.stream(cluster.partition(tp).inSyncReplicas()).anyMatch(node -> node.id() == leader);
    }

    /**
     * The completeness of leadership movement depends on the task state:
     * IN_PROGRESS: done when the leader becomes the destination.
     * ABORTING or DEAD: always considered as done the destination cannot become leader anymore.
     *
     * There should be no other task state seen here.
     */
    private boolean isLeadershipMovementDone(Cluster cluster, TopicPartition tp, ExecutionTask task) {
      LOG.trace("Current leadership task: {}.", task);
      Node leader = cluster.leaderFor(tp);
      switch (task.state()) {
        case IN_PROGRESS:
          return (leader != null && leader.id() == task.proposal().newLeader())
                 || leader == null
                 || !isInIsr(task.proposal().newLeader(), cluster, tp);
        case ABORTING:
        case DEAD:
          return true;
        default:
          throw new IllegalStateException("Should never be here.");
      }

    }

    /**
     * TODO: this method is not used now. It should be used after Kafka server is at 0.11.0+
     * Get the current replica set for a partition.
     */
    private List<Integer> currentReplicas(Cluster cluster, TopicPartition tp) {
      List<Integer> currentReplicas = new ArrayList<>();
      for (Node node : cluster.partition(tp).replicas()) {
        currentReplicas.add(node.id());
      }
      return currentReplicas;
    }

    /**
     * Mark the task as aborting or dead if needed.
     *
     * Ideally, the task should be marked as:
     * 1. ABORTING: when the execution is stopped by the users.
     * 2. ABORTING: When the destination broker is dead so the task cannot make progress, but the source broker is
     *              still alive.
     * 3. DEAD: when any replica in the new replica list is dead. Or when a leader action times out.
     *
     * Currently KafkaController does not support updates on the partitions that is being reassigned. (KAFKA-6034)
     * Therefore once a proposals is written to ZK, we cannot revoke it. So the actual behavior we are using is to
     * set the task state to:
     * 1. IN_PROGRESS: when the execution is stopped by the users. i.e. do nothing but let the task finish normally.
     * 2. DEAD: when the destination broker is dead. i.e. do not block on the execution.
     *
     * @param cluster the kafka cluster
     * @param task the task to check
     * @return true if the task is marked as dead or aborting, false otherwise.
     */
    private boolean maybeMarkTaskAsDeadOrAborting(Cluster cluster, ExecutionTask task) {
      // Only check tasks with IN_PROGRESS or ABORTING state.
      if (task.state() == IN_PROGRESS || task.state() == ABORTING) {
        switch (task.type()) {
          case LEADER_ACTION:
            if (cluster.nodeById(task.proposal().newLeader()) == null) {
              _executionTaskManager.markTaskDead(task);
              LOG.warn("Killing execution for task {} because the target leader is down", task);
              return true;
            } else if (_time.milliseconds() > task.startTime() + LEADER_ACTION_TIMEOUT_MS) {
              _executionTaskManager.markTaskDead(task);
              LOG.warn("Failed task {} because it took longer than {} to finish.", task, LEADER_ACTION_TIMEOUT_MS);
              return true;
            }
            break;

          case REPLICA_ACTION:
            for (int broker : task.proposal().newReplicas()) {
              if (cluster.nodeById(broker) == null) {
                _executionTaskManager.markTaskDead(task);
                LOG.warn("Killing execution for task {} because the new replica {} is down.", task, broker);
                return true;
              }
            }
            break;

          default:
            throw new IllegalStateException("Unknown task type " + task.type());

        }
      }
      return false;
    }

    /**
     * Due to the race condition between the controller and Cruise Control, some of the submitted tasks may be
     * deleted by controller without being executed. We will resubmit those tasks in that case.
     */
    private void maybeReexecuteTasks() {
      List<ExecutionTask> replicaActionsToReexecute =
          new ArrayList<>(_executionTaskManager.inExecutionTasks(ExecutionTask.TaskType.REPLICA_ACTION));
      if (!replicaActionsToReexecute.isEmpty() && ExecutorUtils.partitionsBeingReassigned(_zkUtils).isEmpty()) {
        LOG.info("Reexecuting tasks {}", replicaActionsToReexecute);
        ExecutorUtils.executeReplicaReassignmentTasks(_zkUtils, replicaActionsToReexecute);
      }

      // Only reexecute leader actions if there is no replica actions running.
      if (replicaActionsToReexecute.isEmpty() && ExecutorUtils.ongoingLeaderElection(_zkUtils).isEmpty()) {
        List<ExecutionTask> leaderActionsToReexecute =
            new ArrayList<>(_executionTaskManager.inExecutionTasks(ExecutionTask.TaskType.LEADER_ACTION));
        if (!leaderActionsToReexecute.isEmpty()) {
          LOG.info("Reexecuting tasks {}", leaderActionsToReexecute);
          ExecutorUtils.executePreferredLeaderElection(_zkUtils, leaderActionsToReexecute);
        }
      }
    }
  }
}
