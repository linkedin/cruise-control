/*
 * Copyright 2017 LinkedIn Corp. Licensed under the BSD 2-Clause License (the "License"). See License in the project root for license information.
 */

package com.linkedin.kafka.cruisecontrol.executor;

import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;
import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfig;
import com.linkedin.kafka.cruisecontrol.analyzer.BalancingProposal;
import com.linkedin.kafka.cruisecontrol.common.BalancingAction;
import com.linkedin.kafka.cruisecontrol.common.KafkaCruiseControlThreadFactory;
import com.linkedin.kafka.cruisecontrol.common.MetadataClient;
import com.linkedin.kafka.cruisecontrol.monitor.LoadMonitor;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import java.util.regex.Pattern;
import kafka.utils.ZkUtils;
import org.apache.kafka.clients.Metadata;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.Node;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.utils.Time;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Collection;
import java.util.List;


/**
 * Executor for Kafka GoalOptimizer.
 * <p>
 * The executor class is responsible for talking to the Kafka cluster to execute the rebalance proposals.
 *
 * The executor is not thread safe.
 */
public class Executor {
  private static final Logger LOG = LoggerFactory.getLogger(Executor.class);
  // The execution progress is controlled by the ExecutionTaskManager.
  private final ExecutionTaskManager _executionTaskManager;
  private final MetadataClient _metadataClient;
  private final long _statusCheckingIntervalMs;
  private final ExecutorService _proposalExecutor;
  private final Pattern _excludedTopics;
  private final String _zkConnect;
  private volatile ZkUtils _zkUtils;

  // Some state for external service to query
  private AtomicReference<ExecutorState.State> _state;
  private volatile boolean _stopRequested;
  private volatile int _numTotalPartitionMovements;
  private volatile int _numFinishedPartitionMovements;
  private volatile long _finishedDataMovementInMB;

  /**
   * The executor class that execute the proposals generated by optimizer.
   *
   * @param config The configurations for cruise control.
   */
  public Executor(KafkaCruiseControlConfig config, Time time) {
    _executionTaskManager =
        new ExecutionTaskManager(config.getInt(KafkaCruiseControlConfig.NUM_CONCURRENT_PARTITION_MOVEMENTS_PER_BROKER_CONFIG),
                                 config.getInt(KafkaCruiseControlConfig.NUM_CONCURRENT_LEADER_MOVEMENTS_CONFIG));
    _zkConnect = config.getString(KafkaCruiseControlConfig.ZOOKEEPER_CONNECT_CONFIG);
    _metadataClient = new MetadataClient(config, new Metadata(), -1L, time);
    _statusCheckingIntervalMs = config.getLong(KafkaCruiseControlConfig.EXECUTION_PROGRESS_CHECK_INTERVAL_MS_CONFIG);
    _excludedTopics = Pattern.compile(config.getString(KafkaCruiseControlConfig.TOPICS_EXCLUDED_FROM_PARTITION_MOVEMENT_CONFIG));
    _proposalExecutor =
        Executors.newSingleThreadExecutor(new KafkaCruiseControlThreadFactory("ProposalExecutor", false, LOG));
    _state = new AtomicReference<>(ExecutorState.State.NO_TASK_IN_PROGRESS);
    _stopRequested = false;
  }

  /**
   * Check whether the executor is executing a set of proposals.
   */
  public ExecutorState state() {
    switch (_state.get()) {
      case NO_TASK_IN_PROGRESS:
        return ExecutorState.noTaskInProgress();
      case EXECUTION_STARTED:
        return ExecutorState.executionStarted();
      case LEADER_MOVEMENT_TASK_IN_PROGRESS:
        return ExecutorState.leaderMovementInProgress();
      case REPLICA_MOVEMENT_TASK_IN_PROGRESS:
        ExecutorState currState =
            ExecutorState.replicaMovementInProgress(_numFinishedPartitionMovements,
                                                    _executionTaskManager.remainingPartitionMovements(),
                                                    _executionTaskManager.tasksInProgress(),
                                                    _executionTaskManager.remainingDataToMoveInMB(),
                                                    _finishedDataMovementInMB);
        if (_state.get() == ExecutorState.State.REPLICA_MOVEMENT_TASK_IN_PROGRESS) {
          return currState;
        } else {
          return state();
        }

      default:
        throw new IllegalStateException("Should never be here!");
    }
  }

  /**
   * Kick off the execution.
   */
  public void startExecution(LoadMonitor loadMonitor) {
    if (_state.compareAndSet(ExecutorState.State.NO_TASK_IN_PROGRESS, ExecutorState.State.EXECUTION_STARTED)) {
      _proposalExecutor.submit(new ProposalExecution(loadMonitor));
    } else {
      throw new IllegalStateException("The execution is already in progress.");
    }
  }

  public void stopExecution() {
    if (_state.get() != ExecutorState.State.NO_TASK_IN_PROGRESS) {
      _stopRequested = true;
    }
  }

  /**
   * Shutdown the executor.
   */
  public void shutdown() {
    LOG.info("Shutting down executor.");
    if (_state.get() != ExecutorState.State.NO_TASK_IN_PROGRESS) {
      LOG.warn("Shutdown executor may take long because execution is still in progress.");
    }
    _proposalExecutor.shutdown();

    try {
      _proposalExecutor.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);
    } catch (InterruptedException e) {
      LOG.warn("Interrupted while waiting for anomaly detector to shutdown.");
    }
    LOG.info("Executor shutdown completed.");
  }

  /**
   * Add the given balancing proposals for execution.
   */
  public void addBalancingProposals(Collection<BalancingProposal> proposals,
                                    Collection<Integer> unthrottledBrokers) {
    if (_state.get() != ExecutorState.State.NO_TASK_IN_PROGRESS) {
      throw new IllegalStateException("Cannot add new proposals while the execution is in progress.");
    }
    // Remove any proposal that involves an excluded topic. This should not happen but if it happens we want to
    // detect this and avoid executing the proposals for those topics.
    Iterator<BalancingProposal> iter = proposals.iterator();
    while (iter.hasNext()) {
      BalancingProposal proposal = iter.next();
      if (_excludedTopics.matcher(proposal.topic()).matches()
          && proposal.balancingAction() == BalancingAction.REPLICA_MOVEMENT) {
        LOG.warn("Ignoring balancing proposal {} because the topics is in the excluded topic set {}",
                 proposal, _excludedTopics);
        iter.remove();
      }
    }
    _executionTaskManager.addBalancingProposals(proposals, unthrottledBrokers);
  }

  /**
   * Get the number of total partition movements for this round of execution..
   */
  public int numTotalPartitionMovements() {
    return _state.get() == ExecutorState.State.NO_TASK_IN_PROGRESS ? 0 : _numTotalPartitionMovements;
  }

  /**
   * Get the number of finished partition movements for this round of execution.
   */
  public int numFinishedPartitionMovements() {
    return _state.get() == ExecutorState.State.NO_TASK_IN_PROGRESS ? 0 : _numFinishedPartitionMovements;
  }

  private class ProposalExecution implements Runnable {
    private final LoadMonitor _loadMonitor;
    ProposalExecution(LoadMonitor loadMonitor) {
      _loadMonitor = loadMonitor;
    }

    public void run() {
      LOG.info("Starting executing balancing proposals.");
      execute();
      LOG.info("Execution finished.");
    }

    /**
     * Start the actual execution of the proposals.
     */
    private void execute() {
      _zkUtils = ZkUtils.apply(_zkConnect, 30000, 30000, false);
      try {
        _state.set(ExecutorState.State.REPLICA_MOVEMENT_TASK_IN_PROGRESS);
        moveReplicas();
        // Start leader movements.
        _state.set(ExecutorState.State.LEADER_MOVEMENT_TASK_IN_PROGRESS);
        moveLeaders();
      } catch (Throwable t) {
        LOG.error("Executor got exception during execution", t);
      } finally {
        // Add the null pointer check for unit test.
        if (_loadMonitor != null) {
          _loadMonitor.resumeMetricSampling();
        }
        _stopRequested = false;
        _executionTaskManager.clear();
        KafkaCruiseControlUtils.closeZkUtilsWithTimeout(_zkUtils, 10000);
        _state.set(ExecutorState.State.NO_TASK_IN_PROGRESS);
      }
    }

    private void moveReplicas() {
      _numTotalPartitionMovements = _executionTaskManager.remainingPartitionMovements().size();
      long totalDataToMoveInMB = _executionTaskManager.remainingDataToMoveInMB();
      int partitionsToMove = _numTotalPartitionMovements;
      LOG.info("Starting {} partition movements.", _numTotalPartitionMovements);
      // Exhaust all the pending partition movements.
      while ((partitionsToMove > 0 || _executionTaskManager.hasTaskInProgress()) && !_stopRequested) {
        // Get tasks to execute.
        List<ExecutionTask> tasksToExecute = _executionTaskManager.getPartitionMovementTasks();
        LOG.info("Executor will execute " + tasksToExecute.size() + " task(s)");

        if (!tasksToExecute.isEmpty()) {
          // Execute the tasks.
          ExecutorUtils.executePartitionMovementTasks(_zkUtils, tasksToExecute);
          _executionTaskManager.markTasksInProgress(tasksToExecute);
        }
        // Wait for some partition movements to finish
        waitForExecutionTaskToFinish();
        partitionsToMove = _executionTaskManager.remainingPartitionMovements().size();
        long dataToMove = _executionTaskManager.remainingDataToMoveInMB();
        _numFinishedPartitionMovements =
            _numTotalPartitionMovements - partitionsToMove - _executionTaskManager.tasksInProgress().size();
        _finishedDataMovementInMB = totalDataToMoveInMB - dataToMove;
        LOG.info("{}/{} ({}%) partition movements completed. {}/{} ({}%) MB have been moved.",
                 _numFinishedPartitionMovements, _numTotalPartitionMovements,
                 _numFinishedPartitionMovements * 100 / _numTotalPartitionMovements,
                 _finishedDataMovementInMB, totalDataToMoveInMB,
                 totalDataToMoveInMB == 0 ? 100 : (_finishedDataMovementInMB * 100) / totalDataToMoveInMB);
      }
      // After the partition movement finishes, wait for the controller to clean the reassignment zkPath. This also
      // ensures a clean stop when the execution is stopped in the middle.
      while (!_executionTaskManager.tasksInProgress().isEmpty()) {
        waitForExecutionTaskToFinish();
      }
      LOG.info("Partition movements finished.");
    }

    private void moveLeaders() {
      int numTotalLeaderMovements = _executionTaskManager.remainingLeaderMovements().size();
      LOG.info("Starting {} leader movements.", numTotalLeaderMovements);
      int leaderMoved = 0;
      while (!_executionTaskManager.remainingLeaderMovements().isEmpty() && !_stopRequested) {
        leaderMoved += moveLeadersInBatch();
        LOG.info("{}/{} ({}%) leader movements completed.", leaderMoved, numTotalLeaderMovements,
                 leaderMoved * 100 / numTotalLeaderMovements);
      }
      LOG.info("Leader movements finished.");
    }

    private int moveLeadersInBatch() {
      List<ExecutionTask> leaderMovementTasks = _executionTaskManager.getLeaderMovementTasks();
      int numLeadersToMove = leaderMovementTasks.size();
      LOG.debug("Executing {} leader movements in a batch.", numLeadersToMove);
      // Execute the leader movements.
      if (!leaderMovementTasks.isEmpty() && !_stopRequested) {
        // Mark leader movements in progress.
        _executionTaskManager.markTasksInProgress(leaderMovementTasks);
        // Execute leader movement tasks
        // Ideally we should avoid adjust replica order if not needed, but due to a bug in open source Kafka
        // metadata cache on the broker side, the returned replica list may not match the list in zookeeper.
        // Because reading the replica list from zookeeper would be too expensive, we simply write all the
        // orders to zookeeper and let the controller discard the ones that do not need an update.
        LOG.trace("Adjusting replica orders");
        ExecutorUtils.adjustReplicaOrderBeforeLeaderMovements(_zkUtils, leaderMovementTasks);
        waitForReplicaOrderAdjustmentFinish();

        // Run preferred leader election.
        ExecutorUtils.executePreferredLeaderElection(_zkUtils, leaderMovementTasks);
        LOG.trace("Waiting for leader movement batch to finish.");
        while (!_executionTaskManager.tasksInProgress().isEmpty() && !_stopRequested) {
          waitForExecutionTaskToFinish();
        }
      }
      return numLeadersToMove;
    }

    /**
     * Periodically check to see if the replica order adjustment has finished.
     */
    private void waitForReplicaOrderAdjustmentFinish() {
      while (!ExecutorUtils.partitionsBeingReassigned(_zkUtils).isEmpty() && !_stopRequested) {
        try {
          Thread.sleep(_statusCheckingIntervalMs);
        } catch (InterruptedException e) {
          // let it go
        }
      }
    }

    /**
     * This method periodically check zookeeper to see if the partition reassignment has finished or not.
     */
    private void waitForExecutionTaskToFinish() {
      List<ExecutionTask> finishedTasks = new ArrayList<>();
      do {
        Cluster cluster = _metadataClient.refreshMetadata().cluster();
        for (ExecutionTask task : _executionTaskManager.tasksInProgress()) {
          TopicPartition tp = task.proposal.topicPartition();
          boolean destinationExists = false;
          boolean sourceExists = false;
          if (task.proposal.balancingAction() == BalancingAction.REPLICA_MOVEMENT) {
            for (Node node : cluster.partition(tp).replicas()) {
              destinationExists = destinationExists || (node.id() == task.destinationBrokerId());
              sourceExists = sourceExists || (node.id() == task.sourceBrokerId());
            }
            if (destinationExists && !sourceExists) {
              finishedTasks.add(task);
            }
          } else {
            if (cluster.leaderFor(tp).id() == task.destinationBrokerId()) {
              finishedTasks.add(task);
            }
          }
        }
        if (finishedTasks.size() == 0) {
          maybeReexecuteTasks();
          try {
            Thread.sleep(_statusCheckingIntervalMs);
          } catch (InterruptedException e) {
            // let it go
          }
        }
      } while (finishedTasks.size() == 0 && !_stopRequested);
      // Some tasks has finished, remove them from in progress task map.
      _executionTaskManager.completeTasks(finishedTasks);
      LOG.info("Completed tasks: {}", finishedTasks);
    }

    /**
     * Due to the race condition between the controller and cruise control, some of the submitted tasks may be
     * deleted by controller without being executed. We will resubmit those tasks in that case.
     */
    private void maybeReexecuteTasks() {
      boolean shouldReexecuteTasks = !_executionTaskManager.tasksInProgress().isEmpty() &&
          ExecutorUtils.partitionsBeingReassigned(_zkUtils).isEmpty();
      if (shouldReexecuteTasks) {
        LOG.info("Reexecuting tasks {}", _executionTaskManager.tasksInProgress());
        List<ExecutionTask> tasksToReexecute = new ArrayList<>();
        for (ExecutionTask executionTask : _executionTaskManager.tasksInProgress()) {
          if (executionTask.proposal.balancingAction() == BalancingAction.REPLICA_MOVEMENT) {
            tasksToReexecute.add(executionTask);
          }
        }
        ExecutorUtils.executePartitionMovementTasks(_zkUtils, tasksToReexecute);
      }
    }
  }
}
